# Day 5 â€“ PySpark + SQL Interview Practice (Advanced)

This folder contains advanced and medium-complexity interview-style coding questions solved using Apache Spark (PySpark), SQL (MySQL-compatible), and DataFrame operations.

## âœ… Problems Covered

1. **Find Second Purchase Date for Each Customer**  
   ğŸ“„ `second_purchase.csv`  
   ğŸ§  Use: ROW_NUMBER + filtering

2. **Compute Running Average Revenue per Product**  
   ğŸ“„ `sales_avg.csv`  
   ğŸ§  Use: AVG() over window

3. **Identify Customers with Consecutive Purchase Days**  
   ğŸ“„ `consecutive_purchases.csv`  
   ğŸ§  Use: DATEDIFF + ROW_NUMBER + grouping

4. **Top 2 Products by Total Revenue per Region**  
   ğŸ“„ `top_products.csv`  
   ğŸ§  Use: SUM + DENSE_RANK

5. **Flatten Multi-Item Transactions into Rows**  
   ğŸ“„ `multi_item_txns.csv`  
   ğŸ§  Use: explode() (PySpark), UNNEST/FLATTEN (SQL-like)

6. **Fill in Missing Dates with Zero Revenue**  
   ğŸ“„ `revenue_with_gaps.csv`  
   ğŸ§  Use: sequence() + explode() + join()

7. **Detect Inactive (Churned) Customers Based on 30-Day Inactivity**  
   ğŸ“„ `churn_check.csv`  
   ğŸ§  Use: MAX(purchase_date) + date_diff

## ğŸ“ Folder Structure

```
Day5/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ second_purchase.csv
â”‚   â”œâ”€â”€ sales_avg.csv
â”‚   â”œâ”€â”€ consecutive_purchases.csv
â”‚   â”œâ”€â”€ top_products.csv
â”‚   â”œâ”€â”€ multi_item_txns.csv
â”‚   â”œâ”€â”€ revenue_with_gaps.csv
â”‚   â””â”€â”€ churn_check.csv
â”œâ”€â”€ pyspark/
â”‚   â”œâ”€â”€ second_purchase_date.py
â”‚   â”œâ”€â”€ running_avg_sales.py
â”‚   â”œâ”€â”€ consecutive_purchase_streaks.py
â”‚   â”œâ”€â”€ top_n_products_by_revenue.py
â”‚   â”œâ”€â”€ explode_multi_item_txns.py
â”‚   â”œâ”€â”€ fill_missing_dates.py
â”‚   â””â”€â”€ churned_customers.py
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ second_purchase_date.sql
â”‚   â”œâ”€â”€ running_avg_sales.sql
â”‚   â”œâ”€â”€ consecutive_purchase_streaks.sql
â”‚   â”œâ”€â”€ top_n_products_by_revenue.sql
â”‚   â”œâ”€â”€ explode_multi_item_txns.sql
â”‚   â”œâ”€â”€ fill_missing_dates.sql
â”‚   â””â”€â”€ churned_customers.sql
```

## ğŸ§  Focus Areas

This set focuses on:
- Advanced windowing and ranking
- Time-based pattern detection
- Array flattening and structural transformation
- Filling gaps and identifying inactivity (churn)
- Data cleanup and transformation (e.g., handling string arrays)
- Recursive SQL simulation alternatives (e.g., in explode)

Perfect for interview prep and strengthening PySpark + SQL transformation logic!